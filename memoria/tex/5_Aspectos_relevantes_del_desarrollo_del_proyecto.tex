\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

%Este apartado pretende recoger los aspectos más interesantes del desarrollo del proyecto, %comentados por los autores del mismo.
%Debe incluir desde la exposición del ciclo de vida utilizado, hasta los detalles de mayor %relevancia de las fases de análisis, diseño e implementación.
%Se busca que no sea una mera operación de copiar y pegar diagramas y extractos del código fuente, sino que realmente se justifiquen los caminos de solución que se han tomado, especialmente aquellos que no sean triviales.
%Puede ser el lugar más adecuado para documentar los aspectos más interesantes del diseño y de la %implementación, con un mayor hincapié en aspectos tales como el tipo de arquitectura elegido, los índices de las tablas de la base de datos, normalización y desnormalización, distribución en ficheros3, reglas de negocio dentro de las bases de datos (EDVHV GH GDWRV DFWLYDV), aspectos de desarrollo relacionados con el WWW...
%Este apartado, debe convertirse en el resumen de la experiencia práctica del proyecto, y por sí mismo justifica que la memoria se convierta en un documento útil, fuente de referencia para los autores, los tutores y futuros alumnos.


\section{Carga y almacenamiento de ubicaciones}

El primer paso del desarrollo del proyecto consiste en el almacenamiento de las ubicaciones obtenidas desde un servicio de geolocación como \textit{Open Street Map}. La elección de este sobre otras viene motivada debido principalmente a que se trata de un proyecto completamente abierto sin ninguna restricción monetaria sobre la extracción de datos, además es mantenido por la comunidad proporcionando documentación sobre el uso y la estructura de datos que se manejan.

Para la obtención de las ubicaciones se realizarían peticiones a la API de Overpass, siendo esta perteneciente al proyecto de Open Street Map. Esta es la que se suele utilizar para labores de lectura de datos.

\subsection{Carga inicial de ubicaciones}

Inicialmente se consideró el cargar las ubicaciones de distintas capitales de Europa, incluyéndose entre estas Madrid, París, Roma, Londres, Berlín y Amsterdam. Dado que Open Street Map cuenta con un sistema de etiquetado con formato \textit{clave-valor} sobre los elementos del su modelo de datos se determinar el incluir únicamente aquellos nodos con clave <<amenity>>. Esta se suele utilizar para cubrir diversos establecimientos públicos, servicios y negocios, por lo que se consideró adecuada para los objetivos buscados en el proyecto.

Para introducir las ubicaciones en la base de datos se optó inicialmente por usar la librería \textit{OSMPythonTools}, que provee una interfaz sencilla con la que hacer peticiones a la API. Debido a que el proceso conllevaba cierto tiempo, se empleó una alternativa con el plugin \textit{APOC} que permite cargar datos de la respuesta de una petición HTTP o un fichero JSON.

Una vez el proceso de carga se terminó, contábamos con un total de unos 280.000 nodos y 443 distintos valores de <<amenity>>.


\tablaSmall{Nodos por ciudad en la primera carga de datos}{ |l|c|}{carganodos1}
{\textbf{Ciudad} & \textbf{Número de nodos}\\}{ 
	Madrid & 27.117\\
	París & 54.048\\
	Berlín & 82.900\\
	Londres & 84.558\\
	Roma & 16.675\\
	Amsterdam & 13.214\\
}


Al analizar los datos nos percatamos de que muchas <<amenities>> tenían fallos ortográficos o poseían un significado similar a otros. Este problema es debido a que las distintas claves de \textit{Open Street Map} no cuentan con un diccionario predefinido de posibles valores, sino que son los usuarios que crean las ubicaciones quienes escogen el valor de estas, estén registradas anteriormente o no.

Para resolver este problema se intentaron aplicar algoritmos de distancia de cadenas con el objetivo de unificar valores de <<amenity>>. Esta solución concluyó con que solo a partir de un 95\% de similitud se podrían juntar etiquetas con relativa precisión, aunque esta operación tendría que hacerse con supervisión para evitar posibles errores.

Otra solución que se buscó es utilizar la información de una página auxiliar de \textit{Open Street Map} que cuenta con estadísticas de los valores de las distintas etiquetas. Se intentó unificar las etiquetas a solo aquellas consideradas como <<oficiales>>, siendo estas las que cuentan con una página dentro de la wiki de \textit{Open Street Map}. Esta técnica se acabó desestimando debido a que muchas de las categorías con más aparición entre las ubicaciones cargadas no contaban con página en la wiki, siendo irrelevante este factor en su relevancia.

Adicionalmente se descubrió que gran cantidad de los nodos que estaban cargados en la base de datos pertenecían a mobiliario urbano (bancos, aparcamientos, papeleras, fuentes...), por lo que se consideró su eliminación debido a que no se tenían como tan importantes en comparación con el resto de categorías. Finalmente se desestimó para evitar el caso de un posible sesgo de los datos con su eliminación. Se terminó borrando aquellos nodos que contaban con un valor de \textit{amenity} auxiliar del modelo de datos de \textit{Open Street Map} (Yes, No, Fixme...) debido a que no contaban con ningún significado comercial o similar.


\subsubsection{Enlazado de nodos}

Mientras se encontraba una solución a los problemas enumerados anteriormente se decidió continuar con el enlazado de los nodos en la base de datos. Esto consistía en crear enlaces entre aquellos nodos que estuvieran en un radio de 100 metros. Para hacer esto se dotaron a los nodos con atributos tipo <<Point>>, creados a partir de la latitud y longitud de cada una de las ubicaciones, ya que estos eran valores obligatorios en los nodos de \textit{Open Street Map}.

\imagen{Places}{Ejemplo de creación de enlaces por proximidad}{1}

 Una vez dotados a los nodos de estos atributos se crearon los enlaces entre los nodos con tal proximidad usando utilidades que el lenguaje de consultas Cypher provee para manejar distancias y coordenadas. Uno de los problemas de Neo4j es que posee es que solo permite crear enlaces unidireccionales. Si bien esto al principio resultaba problemático puesto que se creaban enlaces dobles entre cada par de nodos duplicando el número necesario de relaciones, al final se optó por crear un único enlace por par de nodos teniendo en consideración que habría que obviar la dirección en las consultas que se hagan a la base de datos sobre las ubicaciones, reduciendo así el número de aristas del grafo.
 
  Tras finalizar este proceso se completó para todas las ciudades, se saldó el número total de enlaces de proximidad en 5.712.246. Dados los resultados anteriores se consideró que el número de conexiones era bastante superior al esperado y se desestimó el continuar con las ciudades actuales debido al esfuerzo computacional que conllevaría su procesamiento.


\subsection{Segunda carga de nodos}

Una vez vistos los resultados aportados por la elección de las ciudades anteriores se decidió utilizar otras nuevas con un tamaño inferior. Se eligieron Barcelona, Bilbao, Logroño, Oviedo, Santander, Zaragoza, Valladolid, Sevilla, Valencia y Madrid como primera aproximación, con intención de descartar alguna en caso de que se contara con un número muy elevado de ubicaciones.

\tablaSmall{Nodos iniciales por ciudad en la segunda carga de datos}{ |l |c|}{carganodos2ini}
{\textbf{Ciudad} & \textbf{Número de nodos}\\}{ 
	Barcelona & 24.860\\	
	Madrid & 27.186\\	
	Sevilla & 6.323	\\
	Zaragoza & 6.384\\	
	Oviedo & 1.983\\	
	Valladolid & 3.034\\	
	Bilbao & 4.352\\	
	Valencia & 8.154\\	
	Logroño & 1.363\\	
	Santander & 3.038\\	
}

Tras el análisis de las nuevas ciudades reducimos estas a solo 3; Sevilla, Zaragoza y Valencia, para evitar incurrir en el mismo problema que hemos visto anteriormente, centrándonos en alcanzar los objetivos propuestos para el proyecto con estas ciudades antes de incluir otras nuevas.


Una vez determinadas las ciudades con las que trabajar se procedió a realizar el enlazado de nodos como se hizo anteriormente, dándonos un número manejable de relaciones de proximidad, por lo que se continuó con la creación de las redes de interacción entre categorías.

\subsubsection{Interacción entre categorías}

Tras haber creado la red de ubicaciones para cada ciudad el siguiente paso sería el de ver la interacción de las categorías por ciudad. Esta tomaría forma similar a una matriz de adyacencia siendo cada componente el número de veces que los nodos de una categoría poseen enlaces de proximidad con nodos de la misma u otra categoría.

\imagen{InteractionMatrix}{Ejemplo de interacción entre categorías}{1}

Neo4j nos permite obtener mediante consultas la información que contendrían estas matrices de categorías, solo que en una estructura tabular en lugar de matricial, por lo que habría que transformar los datos a una matriz. Un problema que se nos presenta es que en la base de datos no podemos almacenar matrices, ya que Neo4j no soporta este tipo de estructuras, además de sería complicado hacer consultas para obtener datos de esta matriz.

Se intentó buscar alguna forma en la que almacenar los datos de las categorías además de herramientas para operar con ellos en los \textit{plugins} que Neo4j tiene, especialmente GDS (Graph Data Science) por obvias razones. Tras realizar búsquedas en la documentación de dichos \textit{plugins}, así como hacer distintos cursos oficiales que se ofrecen de forma gratuita no se encontró ninguna utilidad que nos sirviera.

Para solventar el problema de la representación de la matriz de categorías se pensó esta como un grafo pesado con enlaces no dirigidos, forma que Neo4j sí que es capaz de manejar. Para ello se crearán nodos <<Categoría>> que contarán como atributos el nombre de la categoría, la ciudad a la que pertenecen y el número de nodos con los que cuenta. En cuanto a los enlaces estos serían no dirigidos (dentro de las limitaciones de Neo4j) con un atributo representativo del número de interacciones entre categorías. Este grafo también contaría con autoenlaces para almacenar la interacción de una categoría consigo misma. 

\imagen{CategoryNetwork}{Ejemplo de red de categorías}{1}

Un problema de esta representación es que normalmente conlleva a crear grafos totalmente conectados y por tanto con un número elevado de enlaces. Pese a las desventajas que esto presenta, se tomó ventaja creando enlaces entre par de categorías aunque estas no interactúen, pero con un atributo de interacción igual a 0. De esta forma se podrán almacenar los coeficientes que nos proporcionen los distintos métodos a aplicar en un atributo del enlace pese a que no exista interacción.

\subsubsection{Aplicación del método \textit{Permutation}}
Una vez obtenidas las redes de categorías para cada ciudad tocaría aplicar unos de los métodos con los que obtendríamos métricas con las que hacer recomendaciones. En este caso el método \textit{Permutation}.

Este método consiste en, sobre la red de ubicaciones de cada ciudad, mantener su estructura (nodos y enlaces) pero permutando las categorías de cada nodo. Una vez permutados se volvería a crear la red de categorías y guardarla. Esto se haría multitud de iteraciones haciendo así una simulación de Monte Carlo. Inicialmente el número de simulaciones que se realizarían sería de 10.000, pero una vez observado el tiempo que conlleva el realizar cada una se redujo a 1000. Todo lo anterior se hizo modificando la red desde Neo4j.

Otra vez más se nos planteó el problema de cómo almacenar la red de categorías de cada iteración de este método, ya que crear una nueva red con nuevos nodos y enlaces supondría una gran carga en cuestión de almacenamiento en la base de datos. Como solución se aprovechó el hecho de que la red de categorías constituye un grafo completamente conectado, almacenando dentro de las propiedades de los enlaces una lista que contenga la interacción entre cada par de categorías en cada iteración del método.

\imagen{PermutationNetwork}{Representación de las simulaciones del método \textit{Permutation}}{0.75}


\subsubsection{Aplicación de cálculo de coeficientes de Jensen}

Dado que para obtener los coeficientes de Jensen no es necesario recurrir a simulaciones de Monte Carlo ni generación de modelos nulos, su aplicación fue más inmediata. El problema que presenta está en su almacenamiento. Dado que los coeficientes no son simétricos i.e. $M_{AB}\neq M_{BA}$ no podemos utilizar los enlaces de anteriores métodos o de la red de interacción para almacenar estos coeficientes, ya que estos se usaban como si fuesen no dirigidos. Para solventar esto se recurrió a crear nuevos enlaces dirigidos con una etiqueta distinta para almacenar los coeficientes como atributos del propio enlaces. Para cada par de categorías contaríamos con dos enlaces con distinta dirección entre cada nodo categoría.

Si bien lo anterior funcionaba, lo cierto es que aumentaba significativamente el número de enlaces presentes en la base de datos.

\imagen{JensenNetwork}{Representación de almacenamiento de coeficientes de Jensen en la base de datos}{0.75}

\subsubsection{Aplicación del método \textit{Rewiring}}
En paralelo a la realización del método de \textit{Permutation} se pensó en una forma de realizar este método desde Neo4j a pesar de su complejidad.

El método del \textit{Rewiring} trata de, dada la red de ubicaciones de una ciudad, conservar los nodos y su categoría, pero crear enlaces aleatorios entre nodos con la condición de que los nodos deben mantener el grado que tenían en la red original. Esto constituye una variante del \textit{Configuration Model}, método que a partir de unos nodos y su grado crea redes aletorias, pero con una diferencia, \textit{Rewiring} no permite autoenlaces. Esto lejos de ser algo banal nos plantea un problema, puesto que la creación de enlaces aleatorios cumpliendo la restricción del grado de los nodos puede llevar a casos donde se requiera de autoenlaces para cumplirse. Además, al igual que en \textit{Permutation} se tendrían que realizar 1.000 simulaciones.

Expuesto lo anterior se desestimó la posibilidad de hacer esto desde Neo4j, requiriendo de un entorno que nos aporte más flexibilidad en las operaciones como el que nos pueda proporcionar un lenguaje de programación. Se hicieron unos prototipos de \textit{Rewiring} desde Python creando redes con distinto número de nodos y grados, así como creando pruebas para comprobar su correcto funcionamiento. Esto se haría al mismo tiempo que el otro método, priorizando más este último debido a que es más sencillo de realizar y menos costoso computacionalmente.


\subsubsection{Análisis de los resultados de los métodos}
Tras la finalización del método \textit{Permutation} se procedió a obtener ciertas estadísticas de los resultados de este método. En primer lugar se calcularía la media, desviación típica y Z-Scores; además de los percentiles 97.5 y 2.5 para cada interacción entre categorías. Con estas medidas podríamos obtener las relaciones significativas entre categorías, siendo estas aquellas cuyo valor real se encuentre por encima del percentil 97.5 o por debajo del 2.5 de los valores obtenidos mediante simulación.

Una vez obtenidas las relaciones más significativas estas mostraban como interacciones más importantes aquellas que incluían elementos del mobiliario urbano (Bancos, papeleras, fuentes, etc.). Lo anterior no era información que consideráramos relevante dado que este trabajo está centrado en tiendas y servicios. Esto es debido a que estos elementos están enormemente sobrerepresentados, llegando a ser más de un tercio de la totalidad de ubicaciones que disponíamos. Inicialmente se estimó el no eliminar estas puesto que podía conllevar un sesgo en los datos, pero dados estos resultados quedó claro que suponían un problema y por tanto debían eliminarse.

Otra conclusión que sacamos tras el análisis de los datos fue que la etiqueta <<amenity>> que utilizábamos para obtener las ubicaciones comerciales no contemplaba
a todo el conjunto de tiendas, solo a una parte, existiendo una etiqueta propia de \textit{Open Street Maps} para este propósito, <<shop>>. Nos decidimos a incluir los nodos con esta etiqueta en la base de datos y volver a iniciar las simulaciones eliminando también las ubicaciones de mobiliario urbano.

Antes de comenzar el proceso anterior también se contempló la posibilidad de utilizar datos de otra API de geolocalización gratuita y dejar de usar \textit{Open Street Map}. Entre las que se consideraron tenemos a MapBox y HereMaps con sus planes gratuitos que ofertaban, ya que no se tratan de proyectos abiertos. Después de investigar sobre su modelo de datos se abandonó la posibilidad de su uso puesto que utilizaban múltiples valores en sus etiquetas, cosa que complicaba enormemente su uso a la hora de estructurar por categorías comerciales, además estaban las limitaciones en sus planes, que podrían suponer un problema en el desarrollo del proyecto.

Tras la inclusión de los nuevos nodos y la eliminación del mobiliario urbano de la base de datos se volvió a comenzar las simulaciones del método \textit{permutation}.
Una vez estas comenzaron nos dimos cuenta que estas tardaban un tiempo significativamente mayor que con los datos anteriores, tanto que resultaba inviable proseguir con ellas. Lo anterior es debido a que se incrementó el tiempo que conllevaba obtener la red de categorías para cada simulación i.e. la interacción entre categorías. La causa más probable es que la inclusión de las ubicaciones con etiqueta <<shop>> prácticamente duplicaba el total de distintas categorías con respecto a los datos anteriores, complicando el recuento de interacciones.

Una vez más se tomó como inviable el continuar con los datos que teníamos en el momento.

\subsection{Tercera carga de nodos}
Dados los hechos anteriores se buscó como solución cargar nodos de ciudades más pequeñas, por lo que se utilizarían datos obtenidos de las ciudades de Castilla y León. 

Una vez cargadas la ubicaciones se hizo un análisis y eliminación de los nodos pertenecientes a mobiliario urbano. El análisis mostraba que algunas ciudades tenían un número significativamente bajo del total de ubicaciones, menor que 300, por lo que se eliminaron de la base de datos al considerarse insuficientes. Al finalizar la carga y depurado de nodos contamos con las ciudades de Valladolid, Burgos, Salamanca, León y Palencia.

\tablaSmall{Nodos por ciudad en la tercera carga de datos}{ |l |c|}{carganodos3}
{\textbf{Ciudad} & \textbf{Número de nodos}\\}{ 
	Palencia & 947\\	
	León & 741\\	
	Salamanca & 3.224	\\
	Valladolid & 3.237\\	
	Burgos &  3.999\\	
}

\subsubsection{Obtención de coeficientes}

Tras la carga de datos se crearon las redes de interacción para obtener los coeficientes del método \textit{Permutation} además de los de \textit{Jensen}, que no se había probado en los intentos anteriores.

En cuanto a \textit{Permutation} se obró como se hizo anteriormente, con un tiempo por simulación asumible, por lo que se obtuvieron los resultados de las simulaciones sin mayor problema. Una vez se analizaron los datos no se detectó ninguna incoherencia con las relaciones significativas señaladas por el método, dándose los resultados como buenos.

\textit{Jensen} suponía un problema que el método anterior no tenía. Este no requiere de simulaciones de Monte Carlo, si no que obtiene los coeficientes a partir de la red de ubicaciones original. El problema viene en que, al contrario que el método de \textit{Permutation}, los coeficientes son diferentes para cada par de categorías según cuál se toma origen o destino. i.e. $M_\text{AB} \neq M_\text{BA}$.



\section{Desarrollo Web}
Uno de los objetivos del proyecto consiste en la creación de una aplicación web que permita visualizar los resultados de los análisis realizados sobre las ciudades que hemos escogido, así como, desde un punto de vista de usuario, poder obtener recomendaciones de categorías comerciales dadas una ubicaciones, que es el propósito último de este trabajo.

Dado que la obtención de los datos empezaba a convertirse en un lastre para el desarrollo del proyecto, se decidió hacer en paralelo la creación de la web tras la finalización de la segunda carga de ubicaciones.

Inicialmente se consideraron distintos \textit{frameworks} web con los que crear la aplicación. Dado que el lenguaje de programación elegido para la realización del trabajo es Python tenemos a nuestra disposición \textit{frameworks} como \textit{Flask}, \textit{FastAPI} y \textit{Django}.

Finalmente se acabó por decantarse por \textit{Flask}, esto es debido a que nos aporta una interfaz más sencilla con una curva de aprendizaje menos pronunciada que \textit{Django}, por lo que se adecua a las necesidades del proyecto, además otro factor a considerar es que ya se tiene experiencia en él puesto que se ha utilizado en la asignatura <<Diseño y Mantenimiento del Software>>. Pese a que no se terminó eligiendo, \textit{Django} tiene como factor diferenciador posee un driver de \textit{Neo4j} que incluye un OGM (\textit{Object Graph Mapper}) que facilita el manejo de las entidades en la base de datos, aunque este no nos sería de gran ayuda en este proyecto en concreto.

Otro factor a tener en consideración es la falta de experiencia que se tiene de desarrollo web en el momento de la creación de esta aplicación. Esto es debido a que durante la carrera no se nos han impartido contenidos relacionados a esta disciplina, si bien en momento puntuales y sin hacer mucho hincapié en ellos. Esto afecta ya que el aprendizaje de las técnicas y herramientas relacionadas con el desarrollo web tendrán un peso importante durante la creación proyecto.

Para solventar la carencia de habilidades en este respecto se recurrió tanto a tutoriales como a documentación de sitios como MDN para el aprendizaje de HTML, JavaScript y CSS. Otro recurso que se utilizó fue un curso gratuito impartido por \textit{Neo4j} en el que se enseña como construir aplicaciones web usando el driver de Neo4j para \textit{Python} y \textit{Flask}. Este último, si bien no enseñaba elementos transversales en el desarrollo web, mostraba cómo usar el driver (aunque en estos momentos ya se conocía su uso) y cómo estructurar un proyecto web en Flask.

\section{Sistema de Recomendación}

Una vez obtenidos los coeficientes de los distintos métodos la siguiente tarea consistiría en calcular los \textit{Quality Indices}. Esto supondría que cada ubicación cargada en la base de datos tenga asociada un valor para categoría y para cada índice de calidad. Una forma de ver esto matricialmente sería una matriz de tamaño (nº de categorías) x (nº de ubicaciones) para cada ciudad. Dado que la base de datos no permite almacenar matrices, una forma equivalente de representarlo sería asociar a cada nodo un <<diccionario>> siendo las claves las categorías y los valores el índice de calidad, todo esto para cada índice de calidad que utilicemos.

Neo4j nos supone un problema en este aspecto ya que los nodos que utiliza funcionan cómo un JSON de un único nivel de profundidad, pudiendo ser las claves únicamente \textit{strings} y los valores listas, tipos primitivos o tipos especiales (fechas, puntos de coordenadas...). Esto nos perjudica ya que no sería posible almacenar de una forma simple los indices de calidad.

Dado lo anterior, se decidió cambiar la aproximación que iba a tomar el proyecto. Si inicialmente se buscaba encontrar la categoría más adecuada para unas ubicaciones, ahora sería, dadas una ubicaciones encontrar cuál es la más adecuada para una categoría determinada. Esto haría que en lugar de almacenar los indices de calidad estos se calcularían <<al vuelo>>, bajo demanda. Lo anterior viene también motivado por los tiempos de cálculo que, haciéndose desde Neo4j conllevaban un tiempo de unos 3 minutos por nodo. Teniendo en cuenta que contamos con un numero de aproximadamente 12.000 nodos resultaría inviable.

Esta forma implicaría que no sería posible el uso combinado de índices de calidad mediante un modelo de inteligencia artificial para realizar recomendaciones. No dispondríamos un conjunto de datos con el que poder entrenar y evaluar la precisión del modelo.

Cambiada la aproximación, se prosiguió con los correspondientes cambios en la web para adaptar la funcionalidad.

Tras un tiempo después centrándose en desarrollo web se encontró una posible solución. Para resolver el problema que suponía el almacenamiento, los índices de calidad se almacenarían como un JSON formateado como \textit{string}. Esto pese a parecer una solución pobre nos solventa los problemas que teníamos anteriormente, pudiendo convertir este \textit{string} de vuelta a JSON desde Python y así obtener los valores que buscábamos. Además dentro del plugin APOC de Neo4j, se incluyen procedimientos que permiten para hacer estas conversiones, ya que, aunque Neo4j no permita almacenar <<maps>> o <<diccionarios>> en sus nodos, permite utilizarlos en su lenguaje de consultas, Cypher.

En cuanto al problema que suponía el tiempo de computo de los indices de calidad se probó a hacer los cálculos en Python en lugar de Neo4j. Para ello se construirían las matrices de coeficientes de calidad y de promedio de vecinos por categorías mediante consultas a la base de datos y almacenándolas en diccionarios de Python, consiguiendo así un tiempo de acceso constante. Uno de los problemas que presenta esta forma es que los cálculos se deben de hacer nodo a nodo en lugar de hacer actualizaciones masivas cómo se podría hacer desde Neo4j. Para cada nodo además de la información contenida en las matrices de promedio de vecinos por categorías y de coeficientes de calidad se tendría que contar con el número de interacciones con las categorías, cosa que se haría también mediante consultas a la base de datos. Una vez hechos los cálculos de los índices de calidad estos se agruparían en un JSON formateado como \textit{string} y se añadirían a la información del nodo en la base de datos.

Una vez completada la alternativa, esta mostraba buenos resultados temporales, llevando alrededor de 0,1 segundos por nodo. A la vista de los resultados se prosiguió con esta forma. Con esto contábamos con los índices de calidad calculados,
permitiéndonos contar con un conjunto de datos sobre el que aplicar un modelo de clasificación.

\subsection{Random Forest}


Obtenidos los índices de calidad para cada una de la ubicaciones contamos con un conjunto de datos con los que poder entrenar un modelo y hacer recomendaciones en base a ubicación.

La elección de \textit{Random Forest} sobre otros modelos de inteligencia artificial viene motivada por el buen rendimiento que presentan los modelos de tipo \textit{ensemble} sobre otros simples. Estos están compuesto por un conjunto de clasificadores en lugar de solo uno, permitiendo reducir el sobre ajuste asegurando un buen rendimiento del clasificador.

\subsubsection{Recomendaciones Locales}

Una parte de las recomendaciones que se harían serían utilizando únicamente datos locales, es decir, de la propia ciudad. Esto implica crear un modelo de \textit{Random Forest} para cada ciudad. Las posibles categorías que se pueden recomendar serían exclusivamente aquellas que la propia ciudad posee.

Para poder evaluar el rendimiento real de los modelos se aplicará una técnica común en el ámbito del \textit{machine learning} conocida como validación cruzada. En nuestro caso realizaremos 100 \textit{folds} para los modelos de cada ciudad.

Tras realizar la validación cruzada se nos arrojan los siguientes resultados:

\imagen{LocalMRR}{\textit{Mean Reciprocal Rank} en las predicciones a nivel local}{1}


Como se puede observar, en comparación al resto de índices de calidad los modelos de \textit{Random Forest} no logran superar a todos, viéndose superados en la mayoría de casos por \textit{QJensen} y \textit{QJensen\_raw} que parecen ser los que mejor rendimiento ofrecen. Pese a lo anterior, sí que consigue obtener resultados considerables.

Otra conclusión que se puede extraer de los resultados es el impacto del tamaño de la ciudad a la hora de obtener buenas predicciones. León y Palencia son las ciudades que consiguen obtener un MRR más alto que el resto siendo estas dos las regiones más pequeñas, en cuanto a nivel de nodos, de las que contamos.
\subsubsection{Transferencia}

A la hora de realizar transferencia de información mediante la utilización de un \textit{Random Forest} se nos presenta un problema con respecto al conjunto de datos a utilizar. Este problema tiene origen en el hecho de que las ciudades tienen conjuntos distintos de categorías, teniendo un total de unas 250 en el conjunto de datos. Esto supone que las ubicaciones de una ciudad no van a tener índices de calidad asociados a categorías que no están presentan en dicha ciudad. Además la diferencia de categorías entre ciudades en algunos casos roza las 100, por lo que de hacerse de esta forma habría muchas ubicaciones con gran parte de índices de calidad desconocidos.

Otro problema derivado que supondría el utilizar al información de todas la ubicaciones que disponemos sería el tamaño del conjunto de datos con el que trabajaríamos. De hacerlo así cada ubicación tendría asociada valores para cada índice de calidad y para cada ubicación, teniendo así unos 1.000 atributos con 12.000 ubicaciones. Esto supone un conjunto de datos bastante grande, especialmente si utilizamos técnicas como la validación cruzada para validar el modelo.

Finalmente se decidió por crear distintos modelos de \textit{Random Forest} para en las que apliquemos transferencia. Para ello se utilizaría una ciudad para entrenar (\textit{source}) el modelo y otra sobre la que realizar las predicciones(\textit{target}). Esto supondría tener 20 modelos distintos, puesto que contamos con 5 ciudades y descontamos los modelos que se utilizan para recomendaciones locales. A la hora de obtener el conjunto de datos de cada modelo se utilizarían únicamente las categorías pertenecientes a la intersección entre las categorías de ese par de ciudades, teniendo a su vez que eliminar filas con categoría fuera de esta, así como filtrar columnas con datos pertenecientes a índices de calidad asociados a categorías que no pertenecen a la intersección.

Tras realizar los entrenamientos y predicciones de los distintos modelos, estos nos arrojan los siguientes resultados:

\imagen{TransferMRR}{\textit{Mean Reciprocal Rank} al aplicar transferencia con \textit{Random Forest}}{0.75}