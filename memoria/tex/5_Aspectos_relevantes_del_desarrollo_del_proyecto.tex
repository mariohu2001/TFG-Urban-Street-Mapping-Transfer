\capitulo{5}{Aspectos relevantes del desarrollo del proyecto}

%Este apartado pretende recoger los aspectos más interesantes del desarrollo del proyecto, %comentados por los autores del mismo.
%Debe incluir desde la exposición del ciclo de vida utilizado, hasta los detalles de mayor %relevancia de las fases de análisis, diseño e implementación.
%Se busca que no sea una mera operación de copiar y pegar diagramas y extractos del código fuente, sino que realmente se justifiquen los caminos de solución que se han tomado, especialmente aquellos que no sean triviales.
%Puede ser el lugar más adecuado para documentar los aspectos más interesantes del diseño y de la %implementación, con un mayor hincapié en aspectos tales como el tipo de arquitectura elegido, los índices de las tablas de la base de datos, normalización y desnormalización, distribución en ficheros3, reglas de negocio dentro de las bases de datos (EDVHV GH GDWRV DFWLYDV), aspectos de desarrollo relacionados con el WWW...
%Este apartado, debe convertirse en el resumen de la experiencia práctica del proyecto, y por sí mismo justifica que la memoria se convierta en un documento útil, fuente de referencia para los autores, los tutores y futuros alumnos.


\section{Carga y almacenamiento de ubicaciones}

El primer paso del desarrollo del proyecto consiste en el almacenamiento de las ubicaciones obtenidas desde un servicio de geolocación como \textit{Open Street Map} ~\cite{OSM}. La elección de este sobre otras alternativas viene motivada debido principalmente a que se trata de un proyecto completamente abierto sin ninguna restricción monetaria sobre la extracción de datos, además es mantenido por la comunidad proporcionando documentación sobre el uso y la estructura de datos que se manejan.

Para la obtención de las ubicaciones se realizarían peticiones a la API de Overpass~\cite{Overpass}, la cual pertenece al proyecto de Open Street Map. Esta es la que se suele utilizar para labores de lectura de datos.

\subsection{Carga inicial de ubicaciones}

Inicialmente se consideró el cargar las ubicaciones de distintas capitales de Europa, incluyéndose entre estas Madrid, París, Roma, Londres, Berlín y Amsterdam. Dado que Open Street Map cuenta con un sistema de etiquetado con formato \textit{clave-valor} sobre los elementos del su modelo de datos~\cite{OSMtags}, se decidió trabajar únicamente con aquellos datos con clave <<amenity>>~\cite{openstreetmapESKeyamenityOpenStreetMap}. Esta clave se suele utilizar para cubrir diversos establecimientos públicos, servicios y negocios, por lo que se consideró adecuada para los objetivos buscados en el proyecto.

Para introducir las ubicaciones en la base de datos se optó inicialmente por usar la librería \textit{OSMPythonTools}~\cite{OSMP}, que provee una interfaz sencilla con la que hacer peticiones a la API. Debido a que el proceso conllevaba cierto tiempo, se empleó una alternativa con el plugin \textit{APOC} ~\cite{APOC}, que permite cargar datos de la respuesta de una petición HTTP o un fichero JSON.

Una vez el proceso de carga se terminó, contábamos con un total de unos 280.000 nodos y 443 distintos valores de <<amenity>>.


\tablaSmall{Nodos por ciudad en la primera carga de datos}{ |l|c|}{carganodos1}
{\textbf{Ciudad} & \textbf{Número de nodos}\\}{ 
	Madrid & 27.117\\
	París & 54.048\\
	Berlín & 82.900\\
	Londres & 84.558\\
	Roma & 16.675\\
	Amsterdam & 13.214\\
}


Al analizar los datos nos percatamos de que muchas <<amenities>> tenían fallos ortográficos o poseían un significado similar a otros. Este problema se deriva de que las distintas claves de \textit{Open Street Map} no cuentan con un diccionario predefinido de posibles valores, sino que son los usuarios que crean las ubicaciones los que escogen el valor de estas, estén registradas anteriormente o no.

Para resolver este problema se intentaron aplicar algoritmos de distancia de cadenas como Jaro-Winkler~\cite{Jaro} o Damerau-Levenshtei ~\cite{Damerau} con el objetivo de unificar valores de <<amenity>>. Esta solución concluyó con que solo a partir de un 95\% de similitud se podrían juntar etiquetas con relativa precisión, aunque esta operación tendría que hacerse con supervisión para evitar posibles errores.

Otra solución que se buscó fue la de utilizar la información de una página auxiliar de \textit{Open Street Map} que cuenta con estadísticas de los valores de las distintas etiquetas. Se intentó unificar las etiquetas a solo aquellas consideradas como <<oficiales>>, siendo estas las que cuentan con una página dentro de la wiki de \textit{Open Street Map} ~\cite{openstreetmapESKeyamenityOpenStreetMap}.Para ello se utilizó una API asociada a Open Street Map llamada TagInfo ~\cite{amenitytaginfo}. Esta técnica se acabó desestimando debido a que muchas de las categorías con más aparición entre las ubicaciones cargadas no contaban con página en la wiki.

Posteriormente se descubrió que gran cantidad de los nodos que estaban cargados en la base de datos pertenecían a mobiliario urbano (bancos, aparcamientos, papeleras, fuentes...), por lo que se consideró su eliminación debido a que estaban representados en la base de datos y podían dificultar la identificación de interacciones entre categorías comerciales. No obstante, en una primera aproximación, se optó por mantenerlos para tratar de no sesgar los datos. Lo único que se hizo fue eliminar los datos de aquellos nodos que contaban con un valor de \textit{amenity} auxiliar del modelo de datos de \textit{Open Street Map} (Yes, No, Fixme...) debido a que no contaban con ningún significado comercial o similar.


\subsubsection{Enlazado de nodos}

Mientras se encontraba una solución a los problemas enumerados anteriormente se decidió continuar con el enlazado de los nodos en la base de datos. Esto consistía en crear enlaces entre aquellos nodos que estuvieran en a una distancia geodésica de 100 metros. Para hacer esto se dotaron a los nodos con atributos tipo <<Point>>, creados a partir de la latitud y longitud de cada una de las ubicaciones, ya que estos eran valores obligatorios en los nodos de \textit{Open Street Map}.

\imagen{Places}{Ejemplo de creación de enlaces por proximidad}{1}

 Una vez dotados a los nodos de estos atributos se crearon los enlaces entre los nodos con tal proximidad usando utilidades que el lenguaje de consultas Cypher provee para manejar distancias y coordenadas. Uno de los problemas de Neo4j es que posee es que solo permite crear enlaces unidireccionales. Si bien esto al principio resultaba problemático puesto que se creaban enlaces dobles entre cada par de nodos duplicando el número necesario de relaciones, al final se optó por crear un único enlace por par de nodos teniendo en consideración que habría que obviar la dirección en las consultas que se hagan a la base de datos sobre las ubicaciones, reduciendo así el número de aristas del grafo.
 
  Tras finalizar este proceso se completó para todas las ciudades, se saldó el número total de enlaces de proximidad en 5.712.246. Dados los resultados anteriores se consideró que el número de conexiones era bastante superior al esperado y se desestimó el continuar con las ciudades actuales debido al esfuerzo computacional que conllevaría su procesamiento.


\subsection{Segunda carga de nodos}

Una vez vistos los resultados aportados por la elección de las ciudades anteriores se decidió utilizar otras nuevas con un tamaño inferior. Se eligieron Barcelona, Bilbao, Logroño, Oviedo, Santander, Zaragoza, Valladolid, Sevilla, Valencia y Madrid como primera aproximación, con intención de descartar alguna en caso de que se contara con un número muy elevado de localizaciones comerciales.

\tablaSmall{Nodos iniciales por ciudad en la segunda carga de datos}{ |l |c|}{carganodos2ini}
{\textbf{Ciudad} & \textbf{Número de nodos}\\}{ 
	Barcelona & 24.860\\	
	Madrid & 27.186\\	
	Sevilla & 6.323	\\
	Zaragoza & 6.384\\	
	Oviedo & 1.983\\	
	Valladolid & 3.034\\	
	Bilbao & 4.352\\	
	Valencia & 8.154\\	
	Logroño & 1.363\\	
	Santander & 3.038\\	
}

Tras el análisis de las nuevas ciudades reducimos estas a solo 3: Sevilla, Zaragoza y Valencia, para evitar incurrir en el mismo problema que hemos visto anteriormente, centrándonos en alcanzar los objetivos propuestos para el proyecto con estas ciudades antes de incluir otras nuevas.


Una vez determinadas las ciudades con las que trabajar se procedió a realizar el enlazado de nodos como se hizo anteriormente, dándonos un número manejable de relaciones de proximidad, por lo que se continuó con la creación de las redes de interacción entre categorías.

\subsubsection{Interacción entre categorías}

Tras haber creado la red de ubicaciones comerciales para cada ciudad, en la que los nodos son los emplazamientos comerciales georreferenciados, el siguiente paso sería el de ver la interacción de las categorías por ciudad. Esta tomaría forma similar a una matriz de adyacencia siendo cada componente el número de veces que los nodos de una categoría poseen enlaces de proximidad con nodos de la misma u otra categoría.

\imagen{InteractionMatrix}{Ejemplo de interacción entre categorías}{1}

Neo4j nos permite obtener mediante consultas la información que contendrían estas matrices de categorías, solo que en una estructura tabular en lugar de matricial, por lo que habría que transformar los datos a una matriz. Un problema que se nos presenta es que en la base de datos no podemos almacenar matrices, ya que Neo4j no soporta este tipo de estructuras. Además, vimos posteriormente que resultaría complicado hacer consultas orientadas a obtener las distintas posiciones de esa matriz.

Se intentó buscar alguna forma en la que almacenar los datos de las categorías además de herramientas para operar con ellos en los \textit{plugins} que Neo4j tiene, especialmente GDS (Graph Data Science) ~\cite{GDS} por razones obvias. Tras realizar búsquedas en la documentación de dichos \textit{plugins}, así como hacer distintos cursos oficiales~\cite{neo4jGDScourse} que se ofrecen de forma gratuita no se encontró ninguna utilidad que nos sirviera.

Para solventar el problema de la representación de la matriz de categorías se pensó esta como un grafo pesado con enlaces no dirigidos, estructura que Neo4J maneja adecuadamente. Para ello, se crearán nodos <<Categoría>> que contarán con atributos con el nombre de la categoría, la ciudad a la que pertenecen y el número de nodos con los que cuenta para dicha ciudad. En cuanto a los enlaces estos serían no dirigidos (dentro de las limitaciones de Neo4j) con un atributo representativo del número de interacciones entre categorías. Este grafo también contaría con autoenlaces para almacenar la interacción de una categoría consigo misma. 

\imagen{CategoryNetwork}{Ejemplo de red de categorías}{1}

Un problema de esta representación es que normalmente conlleva a crear grafos totalmente conectados y por tanto con un número elevado de enlaces. Para solventar esta posible desventaja, si bien se creó por defecto un enlace entre cada par de categorías (incluso si no interactuaban entre ellas), en estos casos se les asignó un peso de la interacción igual a cero. De esta forma se podrán almacenar los coeficientes que nos proporcionen los distintos métodos a aplicar en un atributo del enlace pese a que no exista interacción.

\subsubsection{Aplicación del método \textit{Permutation}}
Una vez obtenidas las redes de categorías para cada ciudad tocaría aplicar unos de los métodos con los que obtendríamos métricas con las que hacer recomendaciones. En este caso el método \textit{Permutation}.

Este método consiste en, sobre la red de ubicaciones de cada ciudad, mantener su estructura (nodos y enlaces) pero permutando las categorías de cada nodo ~\cite{Ahedo2021,RSVAJSSHJG}. Una vez permutados se volvería a crear la red de categorías y guardarla. Esto se haría multitud de iteraciones haciendo así una simulación de Monte Carlo ~\cite{Montecarlo}. Inicialmente el número de simulaciones que se realizarían sería de 10.000, pero una vez observado el tiempo que conlleva el realizar cada una se redujo a 1000. Todo lo anterior se hizo modificando la red desde Neo4j.

Otra vez más se nos planteó el problema de cómo almacenar la red de categorías de cada iteración de este método, ya que crear una nueva red con nuevos nodos y enlaces supondría una gran carga en cuestión de almacenamiento en la base de datos. Como solución se aprovechó el hecho de que la red de categorías constituye un grafo completamente conectado, almacenando dentro de las propiedades de los enlaces una lista que contenga la interacción entre cada par de categorías en cada iteración del método.

\imagen{PermutationNetwork}{Representación de las simulaciones del método \textit{Permutation}}{0.75}


\subsubsection{Aplicación de cálculo de coeficientes de Jensen}

El siguiente paso sería el cálculo de los coeficientes de Jensen. El problema que presenta Jensen viene en relación con el almacenamiento de los coeficientes de interacción. Dado que los coeficientes no son simétricos i.e. $M_{AB}\neq M_{BA}$ no podemos utilizar los enlaces de anteriores métodos o de la red de interacción para almacenar estos coeficientes, ya que estos se usaban como si fuesen no dirigidos. Para solventar esto se recurrió a crear nuevos enlaces dirigidos con una etiqueta distinta, y a almacenar como atributos de estos enlaces los coeficientes de Jensen. Para cada par de categorías contaríamos con dos enlaces con distinta dirección entre cada nodo categoría.

Si bien lo anterior funcionaba, lo cierto es que aumentaba significativamente el número de enlaces presentes en la base de datos.

\imagen{JensenNetwork}{Representación de almacenamiento de coeficientes de Jensen en la base de datos}{0.75}

\subsubsection{Aplicación del método \textit{Rewiring}}
En paralelo a la realización del método de \textit{Permutation}, se pensó en cómo implementar el método de Rewiring en Neo4J, teniendo en cuenta tanto sus funcionalidades disponibles como sus limitaciones.

El método del \textit{Rewiring} funciona de la siguiente manera, dada la red de ubicaciones de una ciudad, conserva la posición de los nodos, su categoría comercial y su grado (número de enlaces), aleatorizando las conexiones, i.e., mantiene el grado pero cambia quién se une con quién. Esto constituye una variante del \textit{Configuration Model} ~\cite{enwiki:1097175572}, método que a partir de unos nodos y su grado crea redes aletorias, pero con una diferencia, \textit{Rewiring} no permite autoenlaces ni enlaces múltiples ~\cite{RSVAJSSHJG}. Esto lejos de ser algo banal nos plantea un problema, puesto que la creación de enlaces aleatorios cumpliendo la restricción del grado de los nodos puede llevar a casos donde se requiera de autoenlaces para cumplirse. Además, al igual que en \textit{Permutation} se tendrían que realizar 1.000 simulaciones.

Expuesto lo anterior se desestimó la posibilidad de hacer esto desde Neo4j, requiriendo de un entorno que nos aporte más flexibilidad en las operaciones como el que nos pueda proporcionar un lenguaje de programación. Se hicieron unos prototipos de \textit{Rewiring} desde Python creando redes con distinto número de nodos y grados, así como creando pruebas para comprobar su correcto funcionamiento. Esto se haría al mismo tiempo que el otro método(Permutation), priorizando más este último debido a que es más sencillo de realizar y menos costoso computacionalmente.


\subsubsection{Análisis de los resultados de los métodos}
Tras la finalización del método \textit{Permutation} se procedió a obtener ciertas estadísticas de los resultados de este método. En primer lugar se calcularía la media, desviación típica y Z-Scores; además de los percentiles 97.5 y 2.5 para cada interacción entre categorías. Con estas medidas podríamos obtener las relaciones significativas entre categorías, siendo estas aquellas cuyo valor real se encuentre por encima del percentil 97.5 o por debajo del 2.5 de los valores obtenidos mediante simulación ~\cite{Ahedo2021}.

Una vez obtenidas las relaciones más significativas estas mostraban como interacciones más importantes aquellas que incluían elementos del mobiliario urbano (Bancos, papeleras, fuentes, etc.). Si bien inicialmente optamos por no eliminar estos elementos de mobiliario urbano para no sesgar los datos, a la luz de estos resultados vimos que se hacía necesario eliminarlos, ya que nuestro objetivo principal es la identificación de las interacciones comerciales, y la sobrerrepresentación en la base de datos de los elementos de mobiliario urbano nos estaba dificultando su correcta detección. Lo anterior no era información que consideráramos relevante dado que este trabajo está centrado en tiendas y servicios. Esto es debido a que estos elementos están enormemente sobrerepresentados, llegando a ser más de un tercio de la totalidad de ubicaciones que disponíamos. Inicialmente se estimó el no eliminar estas puesto que podía conllevar un sesgo en los datos, pero dados estos resultados quedó claro que suponían un problema y por tanto debían eliminarse.

Otra conclusión que sacamos tras el análisis de los datos fue que la etiqueta <<amenity>> que utilizábamos para obtener las ubicaciones comerciales no contemplaba
a todo el conjunto de tiendas, solo a una parte, existiendo una etiqueta propia de \textit{Open Street Maps} para este propósito, <<shop>> ~\cite{OSMshop}. Así pues, decidimos eliminar los elementos del mobiliario urbano y a incluir las ubicaciones con etiqueta <<shop>>, y volvimos a iniciar las simulaciones de nuevo.

Antes de comenzar el proceso anterior también se contempló la posibilidad de utilizar datos de otra API de geolocalización gratuita y dejar de usar \textit{Open Street Map}. Entre las que se consideraron tenemos a MapBox ~\cite{mapboxMapsGeocoding} y HereMaps ~\cite{hereHEREWeGo} con sus planes gratuitos que ofertaban, ya que no se tratan de proyectos abiertos. Después de investigar sobre su modelo de datos se abandonó la posibilidad de su uso puesto que utilizaban múltiples valores en sus etiquetas, cosa que complicaba enormemente su uso a la hora de estructurar por categorías comerciales, además estaban las limitaciones en sus planes, que podrían suponer un problema en el desarrollo del proyecto.

Tras la inclusión de los nuevos nodos y la eliminación del mobiliario urbano de la base de datos se volvió a comenzar las simulaciones del método \textit{permutation}.
Una vez estas comenzaron nos dimos cuenta que estas tardaban un tiempo significativamente mayor que con los datos anteriores, tanto que resultaba inviable proseguir con ellas. Lo anterior es debido a que se incrementó el tiempo que conllevaba obtener la red de categorías para cada simulación i.e. la interacción entre categorías. La causa más probable es que la inclusión de las ubicaciones con etiqueta <<shop>> prácticamente duplicaba el total de distintas categorías con respecto a los datos anteriores, complicando el recuento de interacciones.

Una vez más se tomó como inviable el continuar con los datos que teníamos en el momento.

\subsection{Tercera carga de nodos}
Dados los hechos anteriores se buscó como solución cargar nodos de ciudades más pequeñas, por lo que se utilizarían datos obtenidos de las ciudades de Castilla y León. 

Una vez cargadas la ubicaciones se hizo un análisis y eliminación de los nodos pertenecientes a mobiliario urbano. El análisis mostraba que algunas ciudades tenían un número significativamente bajo del total de ubicaciones, menor que 300, por lo que se eliminaron de la base de datos al considerarse insuficientes. Al finalizar la carga y depurado de nodos contamos con las ciudades de Valladolid, Burgos, Salamanca, León y Palencia.

\tablaSmall{Nodos por ciudad en la tercera carga de datos}{ |l |c|}{carganodos3}
{\textbf{Ciudad} & \textbf{Número de nodos}\\}{ 
	Palencia & 947\\	
	León & 741\\	
	Salamanca & 3.224	\\
	Valladolid & 3.237\\	
	Burgos &  3.999\\	
}

\subsubsection{Obtención de coeficientes}

Tras la carga de datos se crearon las redes de interacción para obtener los coeficientes del método \textit{Permutation} además de los de \textit{Jensen}, que no se había probado en los intentos anteriores.

En cuanto a \textit{Permutation} se obró como se hizo anteriormente, con un tiempo por simulación asumible, por lo que se obtuvieron los resultados de las simulaciones sin mayor problema. Una vez se analizaron los datos no se detectó ninguna incoherencia con las relaciones significativas señaladas por el método, dándose los resultados como buenos.




\section{Desarrollo Web}
Uno de los objetivos del proyecto consiste en la creación de una aplicación web que permita visualizar los resultados de los análisis realizados sobre las ciudades que hemos escogido, así como, desde un punto de vista de usuario, poder obtener recomendaciones de categorías comerciales dadas una ubicaciones, que es el propósito último de este trabajo.

Dado que la obtención de los coeficientes de interacción utilizando como base de cálculo Neo4j estaba suponiendo muchas dificultades, se decidió trabajar en paralelo en la creación de la web tras la finalización de la segunda carga de ubicaciones.

Inicialmente se consideraron distintos \textit{frameworks} web con los que crear la aplicación. Dado que el lenguaje de programación elegido para la realización del trabajo es Python tenemos a nuestra disposición \textit{frameworks} como \textit{Flask}, \textit{FastAPI}~\cite{FastAPI} y \textit{Django}~\cite{Django}.

Finalmente nos decantamos por \textit{Flask}, porque consta de las funcionalidades necesarias para los requisitos del proyecto, cuenta con una curva de aprendizaje menos pronunciada que Django y ya tenía experiencia previa con Flask debido a haberlo utilizado en la asignatura <<Diseño y Mantenimiento del Software>>.

Otro factor a tener en consideración es la falta de experiencia que se tiene de desarrollo web en el momento de la creación de esta aplicación. Esto afecta ya que el aprendizaje de las técnicas y herramientas relacionadas con el desarrollo web tendrán un peso importante durante la creación proyecto.

Para solventar la carencia de habilidades en este respecto se recurrió tanto a tutoriales como a documentación de sitios como MDN~\cite{mozillaDocs} para el aprendizaje de HTML, JavaScript y CSS. Otro recurso que se utilizó fue un curso gratuito impartido por \textit{Neo4j} en el que se enseña como construir aplicaciones web usando el driver de Neo4j para \textit{Python} y \textit{Flask}. Este último, si bien no enseñaba elementos transversales en el desarrollo web, mostraba cómo usar el driver (aunque en estos momentos ya se conocía su uso) y cómo estructurar un proyecto web en Flask.

\section{Sistema de Recomendación}

Una vez obtenidos los coeficientes de interacción entre categorías comerciales de los distintos métodos la siguiente tarea consistiría en calcular los \textit{Quality Indices}. Esto supondría que cada ubicación cargada en la base de datos tenga asociada un valor para categoría y para cada índice de calidad. Una forma de ver esto matricialmente sería una matriz de tamaño (nº de categorías) x (nº de ubicaciones) para cada ciudad. Dado que la base de datos no permite almacenar matrices, una forma equivalente de representarlo sería asociar a cada nodo un <<diccionario>> siendo las claves las categorías y los valores el índice de calidad, todo esto para cada índice de calidad que utilicemos.

Neo4j nos supone un problema en este aspecto ya que los nodos que utiliza funcionan como un JSON de un único nivel de profundidad, pudiendo ser las claves únicamente \textit{strings} y los valores listas, tipos primitivos o tipos especiales (fechas, puntos de coordenadas...). Esto nos perjudica ya que no sería posible almacenar de una forma simple los indices de calidad. Además cabe mencionar los tiempos de cálculo que, haciéndose desde Neo4j conllevaban un tiempo de unos 3 minutos por nodo. Teniendo en cuenta que contamos con un numero de aproximadamente 12.000 nodos, resultaría inviable.

Dado lo anterior, se decidió cambiar el enfoque del sistema de recomendación. Si inicialmente se buscaba encontrar la categoría más adecuada para unas ubicaciones, ahora sería, dadas una ubicaciones encontrar cuál es la más adecuada para una categoría determinada. Esto haría que en lugar de almacenar los indices de calidad estos se calcularían <<al vuelo>>, bajo demanda.

Esta forma implicaría que no sería posible el uso combinado de índices de calidad mediante un modelo de inteligencia artificial para realizar recomendaciones. No dispondríamos un conjunto de datos con el que poder entrenar y evaluar la precisión del modelo.

Cambiada la aproximación, se prosiguió con los correspondientes cambios en la web para adaptar la funcionalidad.

Tras un tiempo después centrándose en desarrollo web se encontró una posible solución. Para resolver el problema que suponía el almacenamiento, los índices de calidad se almacenarían como un JSON formateado como \textit{string}. Esto pese a parecer una solución pobre nos solventa los problemas que teníamos anteriormente, pudiendo convertir este \textit{string} de vuelta a JSON desde Python y así obtener los valores que buscábamos. Además dentro del plugin APOC~\cite{APOC} de Neo4j, se incluyen procedimientos que facilitan hacer estas conversiones, ya que, aunque Neo4j no permita almacenar <<maps>> o <<diccionarios>> en sus nodos, permite utilizarlos en su lenguaje de consultas, Cypher.

En cuanto al problema que suponía el tiempo de computo de los indices de calidad se probó a hacer los cálculos en Python en lugar de Neo4j. Para ello se construirían las matrices de coeficientes de calidad y de promedio de vecinos por categorías mediante consultas a la base de datos y almacenándolas en diccionarios de Python, consiguiendo así un tiempo de acceso constante. Uno de los problemas que presenta esta forma es que los cálculos se deben de hacer nodo a nodo en lugar de hacer actualizaciones masivas cómo se podría hacer desde Neo4j. Para cada nodo además de la información contenida en las matrices de promedio de vecinos por categorías y de coeficientes de calidad se tendría que contar con el número de interacciones con las categorías, cosa que se haría también mediante consultas a la base de datos. Una vez hechos los cálculos de los índices de calidad estos se agruparían en un JSON formateado como \textit{string} y se añadirían a la información del nodo en la base de datos.

Una vez completada la alternativa, esta mostraba buenos resultados temporales, llevando alrededor de 0,1 segundos por nodo.Tras esto contamos con los índices de calidad calculados,
permitiéndonos contar con un conjunto de datos sobre el que aplicar un modelo de clasificación. A la vista de esto se decidió trabajar en paralelo con las dos aproximaciones, proporcionándole al usuario dos enfoques de recomendación alternativos: dadas varias ubicaciones encontrar cuál es la mejor para una determinada categoría, y la alternativa, encontrar cuál es la mejor categoría para una ubicación. 

\subsection{Random Forest}


Obtenidos los índices de calidad para cada una de la ubicaciones contamos con un conjunto de datos con los que poder entrenar un modelo y hacer recomendaciones en base a ubicación.

La elección de \textit{Random Forest} ~\cite{RF} sobre otros modelos de inteligencia artificial viene motivada por el buen rendimiento que presentan los modelos de tipo \textit{ensemble} sobre otros simples ~\cite{Ahedo2021, HundredClass}. Estos están compuesto por un conjunto de clasificadores en lugar de solo uno, evitando correlaciones entre árboles, alcanzando en última instancia un buen compromiso \textit{bias}-varianza.

\subsubsection{Recomendaciones Locales}

Una parte de las recomendaciones que se harían serían utilizando únicamente datos locales, es decir, de la propia ciudad. Esto implica crear un modelo de \textit{Random Forest} para cada ciudad. Las posibles categorías que se pueden recomendar serían exclusivamente aquellas que la propia ciudad posee.

Para poder evaluar el rendimiento real de los modelos se aplicará una técnica común en el ámbito del \textit{machine learning} conocida como validación cruzada. En nuestro caso realizaremos 100 \textit{folds} para los modelos de cada ciudad, para de esta forma evitar distorsionar demasiado la estructura comercial de las ciudades en el conjunto de entrenamiento con los datos utilizados para \textit{test} en cada \textit{fold}.

Tras realizar la validación cruzada se nos arrojan los siguientes resultados:

\imagen{LocalMRR}{\textit{Mean Reciprocal Rank} en las predicciones a nivel local}{1}


Como se puede observar, en comparación al resto de índices de calidad los modelos de \textit{Random Forest} no logran superar a todos, viéndose superados en la mayoría de casos por \textit{QJensen} y \textit{QJensen\_raw} que parecen ser los que mejor rendimiento ofrecen. Pese a lo anterior, sí que consigue obtener resultados considerables.

Otra conclusión que se puede extraer de los resultados es el impacto del tamaño de la ciudad a la hora de obtener buenas predicciones. León y Palencia son las ciudades que consiguen obtener un MRR más alto que el resto siendo estas dos las regiones más pequeñas, en cuanto a nivel de nodos, de las que contamos. Cabe decir que se aprecia que es más fácil predecir y encontrar patrones cuando se cuenta con una menor diversidad comercial.
\subsubsection{Transferencia}

A la hora de realizar transferencia de información mediante la utilización de un \textit{Random Forest} se nos presenta un problema con respecto al conjunto de datos a utilizar. Este problema tiene origen en el hecho de que las ciudades tienen conjuntos distintos de categorías, contando con un total de alrededor de 250 categorías si consideramos todas las ciudades. Esto supone que las ubicaciones de una ciudad no van a tener índices de calidad asociados a categorías que no están presentes en dicha ciudad. Además la diferencia de categorías entre ciudades en algunos casos roza las 100, por lo que de hacerse de esta forma habría muchas ubicaciones con gran parte de índices de calidad desconocidos.

%Otro problema derivado que supondría el utilizar la información de todas la ubicaciones que disponemos sería el tamaño del conjunto de datos con el que trabajaríamos. De hacerlo así cada ubicación tendría asociada valores para cada índice de calidad y para cada ubicación, teniendo así unos 1.000 atributos con 12.000 ubicaciones. Esto supone un conjunto de datos bastante grande, especialmente si utilizamos técnicas como la validación cruzada para validar el modelo.

Finalmente se decidió por crear distintos modelos de \textit{Random Forest} para aplicar transferencia. Para ello se utilizaría una ciudad para entrenar (\textit{source}) el modelo y otra sobre la que realizar las predicciones (\textit{target}). Esto supondría tener 20 modelos distintos, puesto que contamos con 5 ciudades y descontamos los modelos que se utilizan para recomendaciones locales. A la hora de obtener el conjunto de datos de cada modelo se utilizarían únicamente las categorías pertenecientes a la intersección entre las categorías de ese par de ciudades, teniendo a su vez que eliminar filas con categoría fuera de esta, así como filtrar columnas con datos pertenecientes a índices de calidad asociados a categorías que no pertenecen a la intersección.

Tras realizar los entrenamientos y predicciones de los distintos modelos, estos nos arrojan los siguientes resultados:

\imagen{TransferMRR}{\textit{Mean Reciprocal Rank} al aplicar transferencia con \textit{Random Forest}}{0.75}